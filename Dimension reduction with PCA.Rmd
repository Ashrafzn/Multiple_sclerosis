---
title: "Dimension Reduction with PCA"
author: "Fabien Bijsterbosch"
date: "2023-11-22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE}
library(caret)
library(nFactors)
library(EFA.dimensions)
library(stats)
library(factoextra)
library(FactoMineR)
library(ggplot2)
library(patchwork)
library(corrplot)

```



```{r}
#Load in the dataset

sumscores_data <- read.csv("sumscores_output_scaled_lesions_only.csv")
head(sumscores_data, 10)

```


Firstly, we check the correlation of the sumscores. ccording to the literature (James, Witten, Hastie & Tibshirani 2021) Principal Component Analyis(PCA) allows us to summarize a set of highly correlated variables with a smaller number of representative variables.

Furthermore, these representative variables collectively explain most of the variability in the original dataset. The dimension reduction of PCA will then allow us to identify similar groups in a low(er) dimensional space.


```{r}
#Only select the information from the dataset we are interested in. 
# Which are : sumscores of the lipidclasses and the types

sumscores = sumscores_data[, 9:34]
head(sumscores, 10)




```


```{r}
#Correlation matrix.
corrplot(cor(sumscores[, 3:26]), tl.col = "black")


```


We already see that the sumscores seem to be highly positively correlated with each other, thus our PCA would make sense.

```{r}
pr.out = prcomp(sumscores[, 3:26], scale = TRUE)

```

The goal of PCA is to find a low-dimensional representation of our data that captures as much information, which are captured in the principal components. The following table contains the component loadings compared to our original variables where the loading represents the amount of component variance explained
by the variable (e.g., the “correlation” of the variable with the component).
Again, we see a small slice of the data, which contains the loading of the first five variables of the first four components to get a general idea of the PCA. The total PCA contains the full 33 components


```{r}

pr.out$rotation[,1:5]


```



```{r}

var_explained = 100 * pr.out$sdev**2 / sum(pr.out$sdev**2)

par(mfrow = c(1,2))
plot(cumsum(var_explained), type = "o", ylab = "Cumulative PVE", xlab = "Number of components", col = "brown3")
plot(var_explained, type = "o", ylab = "PVE", xlab = "Number of components", col = "lightblue", xlim = c(0, 10))

```


```{r}
summary(pr.out)

```

```{r}
#Parallel analysis

ev = eigen(cor(sumscores[, 3:26]))
ap = parallel(subject = nrow(sumscores[, 3:26]), var = ncol(sumscores[, 3:26]), rep = 1000)
ns = nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(ns, main = "Parallel Analysis")




```


From the parallel analysis we can inspect that Kaiser criterion (eigenvalue >1) would suggest three, however the parallel analysis and Optimal Coordinates both suggest 2. As the Parallel Analysis gives us a more reliable estimate, we take 2 components for our final solution. 


To able to give meaning to these newly derived variables, the loadings will be inspected. It can be inspected by how much "weight" is approximately placed on the orignal variables by the loading vectors.






```{r}

#Getting only the first three principal components
pr.out$rotation[, 1:2]
```
To summarize this table and make it a bit more readable, I do not display certain loadings below a certain threshold, which is 0.2 in this case.

```{r}

rotation_matrix <- pr.out$rotation[, 1:2]  # Assuming you want the first 3 components

# Set the threshold
threshold <- 0.2

# Filter the rotation matrix based on the threshold
filtered_loadings <- round(rotation_matrix, 3)
filtered_loadings[abs(filtered_loadings) < threshold] <- NA  # Replace values below threshold with empty strings

# Display the filtered loadings
rotation_matrix
filtered_loadings


```
We already see that some of the variables can load positive on the first component, whereas some seem to load negative on the first component. The second component seems to be a discrimination component between the variables that loaded low on the first one.

Principal component 1:

- Positive correlations :
PE, PS, PI, LysoPC, LysoPE, LysoPI, NAPE, GPNAE, NAE, DAG, MAG, SM, CER, Cerebroside. 

- Negative correlations
PG...BMP, LysoPS, LysoPG, LysoNAPE, TAG, FFA, Oxylipins, CE.


Principal component 2:
- Positive correlations
LysoPG, LysoNAPE, TAG, Sterols

- Negative correlations
PG...BMP, FFA, LysoNAPE.







